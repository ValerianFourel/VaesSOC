{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"Q_oA1s3XECBw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1729166670746,"user_tz":-120,"elapsed":23766,"user":{"displayName":"Valerian Fourel","userId":"11720891611027180364"}},"outputId":"122816c6-0f34-4658-c63d-7b6f4ab516bb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","source":["import numpy as np\n","import os\n","import torch\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","import dask.dataframe as dd\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.utils.data\n","import random"],"metadata":{"id":"tHOVg8yPe3xL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1729166677166,"user_tz":-120,"elapsed":6423,"user":{"displayName":"Valerian Fourel","userId":"11720891611027180364"}},"outputId":"1c6bb9dc-5df9-4e9c-ac21-d1a590049835"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n","Dask dataframe query planning is disabled because dask-expr is not installed.\n","\n","You can install it with `pip install dask[dataframe]` or `conda install dask`.\n","This will raise in a future version.\n","\n","  warnings.warn(msg, FutureWarning)\n"]}]},{"cell_type":"code","source":["import zipfile\n","\n","# Replace the zip_file_path with the path to the zip file in your Google Drive\n","zip_file_pathElevation = '/content/drive/MyDrive/Colab Notebooks/VaesSOC/Data_Files_Redux/TensorFilesRaster/ElevationTensor_2015_Redux.zip'\n","\n","zip_file_paths = [zip_file_pathElevation]\n","\n","# Replace the destination_folder with the path of the folder where you want to extract the contents\n","destination_folderElevation = '/content/dataElevation'\n","destination_folders = [destination_folderElevation]"],"metadata":{"id":"QJPWVQyfIY9i","executionInfo":{"status":"ok","timestamp":1729166677166,"user_tz":-120,"elapsed":6,"user":{"displayName":"Valerian Fourel","userId":"11720891611027180364"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Unzip the file\n","\n","for i in range(len(destination_folders)):\n","  with zipfile.ZipFile(zip_file_paths[i], 'r') as zip_ref:\n","      zip_ref.extractall(destination_folders[i])\n"],"metadata":{"id":"6DfqQr97e7eB","executionInfo":{"status":"ok","timestamp":1729166702381,"user_tz":-120,"elapsed":25220,"user":{"displayName":"Valerian Fourel","userId":"11720891611027180364"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["sampleCoordinatesElevationIDArrayPositionDf_file = '/content/drive/MyDrive/Colab Notebooks/VaesSOC/Data_Files_Redux/CoordinatesPoints/sampleCoordinatesElevationIDArrayPositionDf_Redux.parquet'\n","sampleCoordinatesElevationIDArrayPositionDf = dd.read_parquet(sampleCoordinatesElevationIDArrayPositionDf_file).compute()\n"],"metadata":{"id":"bKchvQGHe-Yw","executionInfo":{"status":"ok","timestamp":1729166705335,"user_tz":-120,"elapsed":2959,"user":{"displayName":"Valerian Fourel","userId":"11720891611027180364"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["# Part 2 Custom Dataset"],"metadata":{"id":"BXIHPetJIyYq"}},{"cell_type":"code","source":["\n","class CustomRasterDataset(torch.utils.data.Dataset):\n","    'Characterizes a dataset for PyTorch'\n","    def __init__(self, dataFrame, file_path, file_extension,windowSize,re_scale = False,new_min = -1,new_max = 1):\n","        'Initialization'\n","        self.re_scale = re_scale\n","        self.new_min  = new_min\n","        self.new_max = new_max\n","        self.dataFrame = dataFrame\n","        self.file_path = file_path\n","        self.file_extension = file_extension\n","        self.windowSize = windowSize\n","        self.offset = self.windowSize // 2\n","\n","\n","    def __len__(self):\n","        'Denotes the total number of samples'\n","        return len(self.dataFrame)\n","\n","    def __getitem__(self, index):\n","        'Generates one sample of data'\n","        # Select sample\n","        ID =  self.dataFrame.iloc[index]['ID'] # str(ID).rstrip('.0')\n","\n","        x = self.dataFrame.iloc[index]['x'] + random.choice([-2, -1, 0, 1, 2])\n","        y = self.dataFrame.iloc[index]['y'] + random.choice([-2, -1, 0, 1, 2])\n","\n","        # Load data and get label\n","        fullArray = torch.load(self.file_path+ID+self.file_extension)\n","        # Determine the window for the square\n","        left = x - self.offset\n","        right = x + (self.offset + 1)\n","        top = y - self.offset\n","        bottom = y + (self.offset + 1)\n","        X = fullArray[left:right,top:bottom].clone().detach()  # Access value in gpu_dictElevation\n","        # if self.re_scale:\n","          # X =  re_scale(X,self.new_min,self.new_max)\n","        return X.unsqueeze(0)"],"metadata":{"id":"Px8WOkYyIseD","executionInfo":{"status":"ok","timestamp":1729166705336,"user_tz":-120,"elapsed":11,"user":{"displayName":"Valerian Fourel","userId":"11720891611027180364"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["file_pathElevation = '/content/dataElevation/'\n"],"metadata":{"id":"Dn8wHAhWfXvg","executionInfo":{"status":"ok","timestamp":1729166705336,"user_tz":-120,"elapsed":10,"user":{"displayName":"Valerian Fourel","userId":"11720891611027180364"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["batch_sizeElevation = 32"],"metadata":{"id":"znAmNUczfbtE","executionInfo":{"status":"ok","timestamp":1729166705336,"user_tz":-120,"elapsed":9,"user":{"displayName":"Valerian Fourel","userId":"11720891611027180364"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["file_extension = '.pt'\n","num_workers = 2"],"metadata":{"id":"aSnUlKfUfZqH","executionInfo":{"status":"ok","timestamp":1729166705336,"user_tz":-120,"elapsed":8,"user":{"displayName":"Valerian Fourel","userId":"11720891611027180364"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["windowSizeElevation = 65\n","\n","# Create the dataset instance\n","datasetElevation = CustomRasterDataset(sampleCoordinatesElevationIDArrayPositionDf, file_pathElevation, file_extension,windowSizeElevation)"],"metadata":{"id":"dzFqPJUnfNbp","executionInfo":{"status":"ok","timestamp":1729166705337,"user_tz":-120,"elapsed":9,"user":{"displayName":"Valerian Fourel","userId":"11720891611027180364"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["\n","# Create a DataLoader for batching and parallel data loading (you can adjust batch_size and num_workers as needed)\n","\n","dataLoaderElevation = DataLoader(datasetElevation, batch_size=batch_sizeElevation, num_workers=num_workers, shuffle=True)\n","\n","# Now you can use dataLoaderEvapo in your training loop to efficiently access the elevation values in gpu_dictElevation.\n"],"metadata":{"id":"cJOHungAfSHM","executionInfo":{"status":"ok","timestamp":1729166705337,"user_tz":-120,"elapsed":8,"user":{"displayName":"Valerian Fourel","userId":"11720891611027180364"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# Assuming you have already created the 'elevation_dataloader' as mentioned in the previous steps\n","\n","# Get the first batch from the dataloader using the 'next' function\n","first_batch = next(iter(dataLoaderElevation))\n","second_batch = next(iter(dataLoaderElevation))\n","# Print the content of the first batch\n","print(\"First Batch:\")\n","print(first_batch.shape)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K9CP_y_bfhrq","executionInfo":{"status":"ok","timestamp":1729166710904,"user_tz":-120,"elapsed":5574,"user":{"displayName":"Valerian Fourel","userId":"11720891611027180364"}},"outputId":"d7700530-2ce1-4988-e6dc-2578a2011c2e"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-6-59023ef0d4a7>:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  fullArray = torch.load(self.file_path+ID+self.file_extension)\n","<ipython-input-6-59023ef0d4a7>:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  fullArray = torch.load(self.file_path+ID+self.file_extension)\n","<ipython-input-6-59023ef0d4a7>:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  fullArray = torch.load(self.file_path+ID+self.file_extension)\n","<ipython-input-6-59023ef0d4a7>:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  fullArray = torch.load(self.file_path+ID+self.file_extension)\n"]},{"output_type":"stream","name":"stdout","text":["First Batch:\n","torch.Size([32, 1, 65, 65])\n"]}]},{"cell_type":"markdown","source":["# Part 3. Architecture of the Elevation VAE"],"metadata":{"id":"LwZFg-xTNGdL"}},{"cell_type":"code","source":["import torch.optim as optim"],"metadata":{"id":"YoOItC8jfqhW","executionInfo":{"status":"ok","timestamp":1729166710904,"user_tz":-120,"elapsed":10,"user":{"displayName":"Valerian Fourel","userId":"11720891611027180364"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["def count_parameters(model: nn.Module) -> int:\n","    \"\"\"\n","    Count the number of parameters in a PyTorch model.\n","\n","    Args:\n","        model (nn.Module): PyTorch model.\n","\n","    Returns:\n","        int: Total number of parameters.\n","    \"\"\"\n","    return sum(p.numel() for p in model.parameters())\n"],"metadata":{"id":"_i2IcfNbDntl","executionInfo":{"status":"ok","timestamp":1729166710905,"user_tz":-120,"elapsed":9,"user":{"displayName":"Valerian Fourel","userId":"11720891611027180364"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["\n","class ResDown(nn.Module):\n","    \"\"\"\n","    Residual down sampling block for the encoder\n","    \"\"\"\n","\n","    def __init__(self, channel_in, channel_out, kernel_size=3):\n","        super(ResDown, self).__init__()\n","        self.conv1 = nn.Conv2d(channel_in, channel_out // 2, kernel_size, 2, kernel_size // 2)\n","        self.bn1 = nn.BatchNorm2d(channel_out // 2, eps=1e-4)\n","        self.conv2 = nn.Conv2d(channel_out // 2, channel_out, kernel_size, 1, kernel_size // 2)\n","        self.bn2 = nn.BatchNorm2d(channel_out, eps=1e-4)\n","\n","        self.conv3 = nn.Conv2d(channel_in, channel_out, kernel_size, 2, kernel_size // 2)\n","\n","        self.act_fnc = nn.ELU()\n","\n","    def forward(self, x):\n","        skip = self.conv3(x)\n","        x = self.act_fnc(self.bn1(self.conv1(x)))\n","        x = self.conv2(x)\n","        return self.act_fnc(self.bn2(x + skip))\n","\n","\n","class ResUp(nn.Module):\n","    \"\"\"\n","    Residual up sampling block for the decoder\n","    \"\"\"\n","\n","    def __init__(self, channel_in, channel_out, kernel_size=3, scale_factor=2):\n","        super(ResUp, self).__init__()\n","\n","        self.conv1 = nn.Conv2d(channel_in, channel_in // 2, kernel_size, 1, kernel_size // 2)\n","        self.bn1 = nn.BatchNorm2d(channel_in // 2, eps=1e-4)\n","        self.conv2 = nn.Conv2d(channel_in // 2, channel_out, kernel_size, 1, kernel_size // 2)\n","        self.bn2 = nn.BatchNorm2d(channel_out, eps=1e-4)\n","\n","        self.conv3 = nn.Conv2d(channel_in, channel_out, kernel_size, 1, kernel_size // 2)\n","\n","        self.up_nn = nn.Upsample(scale_factor=scale_factor, mode=\"nearest\")\n","\n","        self.act_fnc = nn.ELU()\n","\n","    def forward(self, x):\n","        x = self.up_nn(x)\n","        skip = self.conv3(x)\n","        x = self.act_fnc(self.bn1(self.conv1(x)))\n","        x = self.conv2(x)\n","\n","        return self.act_fnc(self.bn2(x + skip))\n"],"metadata":{"id":"pA6hp4syNKK7","executionInfo":{"status":"ok","timestamp":1729166710905,"user_tz":-120,"elapsed":8,"user":{"displayName":"Valerian Fourel","userId":"11720891611027180364"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["\n","# Define the encoder network\n","class ElevationEncoder(nn.Module):\n","    def __init__(self, latent_dim):\n","        super(ElevationEncoder, self).__init__()\n","        self.latent_dim = latent_dim\n","        self.conv_in = nn.Conv2d(1, 4, 7, 1, 3)\n","        self.res_down_block1 = ResDown(4, 8)\n","        self.res_down_block2 = ResDown(8, 16)\n","        self.res_down_block3 = ResDown(16,32)\n","        self.res_down_block4 = ResDown(32, 64)\n","        self.conv_mu = nn.Conv2d(64, latent_dim, 5, 1)\n","        self.conv_log_var = nn.Conv2d(64, latent_dim, 5, 1)\n","        self.act_fnc = nn.ELU()\n","\n","\n","    def forward(self, x):\n","        x = self.act_fnc(self.conv_in(x))\n","        x = self.res_down_block1(x)  # 32\n","        x = self.res_down_block2(x)  # 16\n","        x = self.res_down_block3(x)  # 8\n","        x = self.res_down_block4(x)\n","        mu = self.conv_mu(x)  # 1\n","        logvar = self.conv_log_var(x)  # 1\n","\n","        return mu, logvar\n","\n","# Define the decoder network\n","class ElevationDecoder(nn.Module):\n","    def __init__(self, latent_dim):\n","        super(ElevationDecoder, self).__init__()\n","        self.latent_dim = latent_dim\n","        self.conv_t_up = nn.ConvTranspose2d(latent_dim, 128, 4, 1)\n","        self.res_up_block1 = ResUp(128, 64)\n","        self.res_up_block2 = ResUp(64, 32)\n","        self.res_up_block3 = ResUp(32,16)\n","        self.res_up_block4 = ResUp(16,8)\n","        self.res_up_block5 = ResUp(8,7)\n","\n","        self.res_down_block1 = ResDown(7,16)\n","\n","\n","        self.conv_out1 = nn.Conv2d(16, 1, 6, stride=1, padding=3)\n","        # self.conv_out3 = nn.Conv2d(2, 1, 3, 1, 1)\n","\n","        self.act_fnc = nn.ELU()\n","        self.act_fnc2 = nn.ELU()\n","\n","\n","    def forward(self, x):\n","        x = x.view(x.shape[0], self.latent_dim, 1, 1)\n","        x = self.act_fnc(self.conv_t_up(x))  # 4\n","        x = self.res_up_block1(x)  # 8\n","        x = self.res_up_block2(x)  # 16\n","        x = self.res_up_block3(x)  # 32\n","        x = self.res_up_block4(x)  # 32\n","        x = self.res_up_block5(x)  # 32\n","        x = self.res_down_block1(x)  # 32\n","\n","        x = self.conv_out1(x)\n","        return x\n","\n","# Combine the encoder and decoder to form the VAE\n","class ElevationVAE(nn.Module):\n","    def __init__(self, latent_dim):\n","        super(ElevationVAE, self).__init__()\n","        self.latent_dim = latent_dim\n","        self.encoder = ElevationEncoder(latent_dim)\n","        self.decoder = ElevationDecoder(latent_dim)\n","\n","    def encode(self, x):\n","        return self.encoder(x)\n","\n","    def decode(self, z):\n","        return self.decoder(z)\n","\n","    def reparameterize(self, mu, logvar):\n","        std = torch.exp(0.5 * logvar)\n","        eps = torch.randn_like(std)\n","        z = mu + eps * std\n","        return z\n","\n","    def forward(self, x):\n","        mu, logvar = self.encode(x)\n","        z = self.reparameterize(mu, logvar)\n","        reconstructed_x = self.decode(z)\n","        return reconstructed_x, mu, logvar\n","\n","\n","# Instantiate the VAE with the desired latent_dim\n","latent_dim = 25\n","vae = ElevationVAE(latent_dim)\n","\n","# Pass the input batch through the VAE\n","reconstructed_batch, mu, logvar = vae(first_batch)\n","\n","# Check the output shape\n","print(\"Reconstructed batch shape:\", reconstructed_batch.shape)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zJ7KYTzmf1tc","executionInfo":{"status":"ok","timestamp":1729166711851,"user_tz":-120,"elapsed":953,"user":{"displayName":"Valerian Fourel","userId":"11720891611027180364"}},"outputId":"ca6c4248-46f5-4ead-c737-f96fbd16dcc9"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Reconstructed batch shape: torch.Size([32, 1, 65, 65])\n"]}]},{"cell_type":"code","source":["\n","# Instantiate the VAE with the desired latent_dim\n","latent_dim = 25\n","vae = ElevationVAE(latent_dim)\n"],"metadata":{"id":"FYWJ6IzBp8Av","executionInfo":{"status":"ok","timestamp":1729166711851,"user_tz":-120,"elapsed":7,"user":{"displayName":"Valerian Fourel","userId":"11720891611027180364"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"-CLPW5G_DrNt","executionInfo":{"status":"ok","timestamp":1729166711852,"user_tz":-120,"elapsed":7,"user":{"displayName":"Valerian Fourel","userId":"11720891611027180364"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["# Pass the input batch through the VAE\n","reconstructed_batch, mu, logvar = vae(first_batch)\n","\n","# Check the output shape\n","print(\"Reconstructed batch shape:\", reconstructed_batch.shape)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_Z1fCuQFp9pm","executionInfo":{"status":"ok","timestamp":1729166712684,"user_tz":-120,"elapsed":839,"user":{"displayName":"Valerian Fourel","userId":"11720891611027180364"}},"outputId":"b72eff3d-8225-4ce1-ec25-02bd5aeacbfe"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Reconstructed batch shape: torch.Size([32, 1, 65, 65])\n"]}]},{"cell_type":"code","source":["count_parameters(vae)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uDmWnQ35Dr2L","executionInfo":{"status":"ok","timestamp":1729166712684,"user_tz":-120,"elapsed":11,"user":{"displayName":"Valerian Fourel","userId":"11720891611027180364"}},"outputId":"882433a3-5a8d-465a-c1a0-fb5509965d27"},"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["443491"]},"metadata":{},"execution_count":19}]},{"cell_type":"markdown","source":["# Part4 Training the VAE"],"metadata":{"id":"86YB0R2yhv1-"}},{"cell_type":"code","source":["loss_weight = len(sampleCoordinatesElevationIDArrayPositionDf)/batch_sizeElevation\n","loss_weight"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EtyPIlHvDFb8","executionInfo":{"status":"ok","timestamp":1729166712685,"user_tz":-120,"elapsed":10,"user":{"displayName":"Valerian Fourel","userId":"11720891611027180364"}},"outputId":"223508c2-5ee8-4f6a-957a-acd8301399df"},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["30666.65625"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["import torch.nn as nn"],"metadata":{"id":"kJWeF2zChx8x","executionInfo":{"status":"ok","timestamp":1729166712685,"user_tz":-120,"elapsed":7,"user":{"displayName":"Valerian Fourel","userId":"11720891611027180364"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["%%capture\n","!pip install torchmetrics"],"metadata":{"id":"vPnWFkvhhz1k","executionInfo":{"status":"ok","timestamp":1729166718370,"user_tz":-120,"elapsed":5691,"user":{"displayName":"Valerian Fourel","userId":"11720891611027180364"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["from torchmetrics.image.lpip import LearnedPerceptualImagePatchSimilarity"],"metadata":{"id":"Z1xQHz89h1kO","executionInfo":{"status":"ok","timestamp":1729166722598,"user_tz":-120,"elapsed":4235,"user":{"displayName":"Valerian Fourel","userId":"11720891611027180364"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["lpips = LearnedPerceptualImagePatchSimilarity(net_type='squeeze')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tkGzSHDfh3iT","executionInfo":{"status":"ok","timestamp":1729166726299,"user_tz":-120,"elapsed":3705,"user":{"displayName":"Valerian Fourel","userId":"11720891611027180364"}},"outputId":"cff8e42a-79ab-4f9d-9dcf-61c2a8f14226"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/squeezenet1_1-b8a52dc0.pth\" to /root/.cache/torch/hub/checkpoints/squeezenet1_1-b8a52dc0.pth\n","100%|██████████| 4.73M/4.73M [00:00<00:00, 62.0MB/s]\n","/usr/local/lib/python3.10/dist-packages/torchmetrics/functional/image/lpips.py:325: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  self.load_state_dict(torch.load(model_path, map_location=\"cpu\"), strict=False)\n"]}]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"F7IZGLA-h7c1","executionInfo":{"status":"ok","timestamp":1729166726300,"user_tz":-120,"elapsed":8,"user":{"displayName":"Valerian Fourel","userId":"11720891611027180364"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["def normalize_tensor01(tensor):\n","    max_val = torch.max(tensor)\n","    min_val = torch.min(tensor)\n","\n","    # Avoid division by zero\n","    if max_val - min_val != 0:\n","        normalized_tensor = (tensor - min_val) / (max_val - min_val)\n","    else:\n","        normalized_tensor = tensor - min_val\n","\n","    return normalized_tensor\n","\n","\n","def normalized_mse(tensor1, tensor2):\n","    # Normalize the tensors\n","    norm_tensor1 = normalize_tensor01(tensor1)\n","    norm_tensor2 = normalize_tensor01(tensor2)\n","\n","    # Compute MSE\n","    mse_loss_normalized = F.mse_loss(norm_tensor1, norm_tensor2)\n","    mse_loss =  F.mse_loss(tensor1, tensor2)\n","\n","    return mse_loss_normalized, mse_loss"],"metadata":{"id":"06r8ciXbGubA","executionInfo":{"status":"ok","timestamp":1729166726300,"user_tz":-120,"elapsed":7,"user":{"displayName":"Valerian Fourel","userId":"11720891611027180364"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["\n","class ElevationVAELoss(nn.Module):\n","    def __init__(self):\n","        super(ElevationVAELoss, self).__init__()\n","\n","\n","    def forward(self, reconstructed_x, x_final, x, mu, logvar,lpips):\n","        # Repeat the last two dimensions three times\n","        reconstructed_x_repeated =  normalize_tensor01(x_final).repeat(1, 3, 1, 1)\n","        x_repeated = normalize_tensor01(x).repeat(1, 3, 1, 1)\n","\n","        # Compute the Mean Squared Error (MSE) reconstruction loss\n","        lpips = lpips(reconstructed_x_repeated, x_repeated)\n","        # Create the L1 loss function\n","        loss_value_l1 = nn.MSELoss(reduction='mean')(reconstructed_x,x)\n","        # Compute the KL divergence term\n","        kl_divergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","        # Return the sum of the reconstruction loss and KL divergence term\n","        return kl_divergence +  (loss_value_l1)*loss_weight + lpips*loss_weight*100"],"metadata":{"id":"Exu1HNuwiAWi","executionInfo":{"status":"ok","timestamp":1729166726300,"user_tz":-120,"elapsed":6,"user":{"displayName":"Valerian Fourel","userId":"11720891611027180364"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["import torch.nn.init as init\n","\n","def reset_parameters(model):\n","    for module in model.modules():\n","        if isinstance(module, (nn.Conv2d, nn.Linear)):\n","            # Reset the weights and biases of Conv2d and Linear layers\n","            module.reset_parameters()\n","        elif isinstance(module, nn.BatchNorm2d):\n","            # Reset the running statistics of BatchNorm2d layers\n","            module.reset_running_stats()\n","\n","# Assuming you have already instantiated the VAE\n","vae = ElevationVAE(latent_dim)\n","\n","# Reinitialize the VAE's parameters\n","reset_parameters(vae)\n"],"metadata":{"id":"CxHq-rqEiX96","executionInfo":{"status":"ok","timestamp":1729166726600,"user_tz":-120,"elapsed":306,"user":{"displayName":"Valerian Fourel","userId":"11720891611027180364"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["learning_rateElevation = 0.0005"],"metadata":{"id":"pAlXoBIfiZ6Z","executionInfo":{"status":"ok","timestamp":1729166726601,"user_tz":-120,"elapsed":6,"user":{"displayName":"Valerian Fourel","userId":"11720891611027180364"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["latent_dimElevation = 25\n"],"metadata":{"id":"NI6xgznnif4p","executionInfo":{"status":"ok","timestamp":1729166726601,"user_tz":-120,"elapsed":5,"user":{"displayName":"Valerian Fourel","userId":"11720891611027180364"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["\n","# Define the function to train the VAE\n","def train_vae(vae, dataloader, num_epochs, learning_rate):\n","    # Set the model to training mode\n","    vae.train()\n","\n","    # Define the Mean Squared Error (MSE) loss function\n","    criterion = ElevationVAELoss()\n","    mse = torch.nn.MSELoss()\n","\n","    # Define the optimizer (you can experiment with different optimizers)\n","    optimizer = optim.Adam(vae.parameters(), lr=learning_rate)\n","    total_batches = len(dataloader)\n","    batches_done = 0\n","    lpips = LearnedPerceptualImagePatchSimilarity(net_type='squeeze').to(device)\n","\n","    for epoch in range(num_epochs):\n","        total_loss = 0.0\n","        batches_done =0\n","        mse_loss = 0\n","        for batch_idx, data in enumerate(dataloader):\n","            # Get the batch of data and move it to the device (e.g., GPU if available)\n","            # inputs = data\n","            dimensions = data.shape\n","\n","            inputs = data.to(device)\n","\n","            # Zero the gradients\n","            optimizer.zero_grad()\n","            # Forward pass\n","            reconstructed_batch, mu, logvar = vae(inputs)\n","            # Compute the MSE loss\n","            loss = criterion(reconstructed_batch, reconstructed_batch, inputs,mu, logvar,lpips)\n","            # Backward pass\n","            loss.backward()\n","\n","\n","            # Update the parameters\n","            optimizer.step()\n","\n","            # Update the total loss for the epoch\n","            total_loss += loss.item()\n","\n","            # Update the number of batches processed\n","            batches_done += 1\n","            mse_loss += mse(normalize_tensor01(reconstructed_batch),normalize_tensor01(inputs))\n","\n","            # Print the progress when a tenth of the epoch is completed\n","            if batches_done % (len(dataloader) // 10) == 0:\n","                print(f\"Epoch [{epoch+1}/{num_epochs}] - Progress: {batches_done}/{len(dataloader)} - Total Loss: {total_loss / (len(dataloader) // 10)}, {mse_loss.item() / (len(dataloader) // 10)}\")\n","                total_loss = 0\n","                mse_loss = 0\n","\n","        # Print the average loss for the epoch\n","        print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {total_loss / ((len(dataloader) - len(dataloader) //10 * 9 ) % 10)}\")\n"],"metadata":{"id":"8KeLj7-zis-l","executionInfo":{"status":"ok","timestamp":1729166726601,"user_tz":-120,"elapsed":5,"user":{"displayName":"Valerian Fourel","userId":"11720891611027180364"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["\n","# Example usage:'device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")'\n","# Assuming you have the training data in 'train_dataloader' and a device set, e.g.,\n","# Instantiate the VAE with the desired latent_dim\n","vae = ElevationVAE(latent_dimElevation).to(device)\n","\n","# Define the number of epochs and learning rate\n","num_epochs = 3\n"],"metadata":{"id":"RVpSfFA6iwNK","executionInfo":{"status":"ok","timestamp":1729166727197,"user_tz":-120,"elapsed":600,"user":{"displayName":"Valerian Fourel","userId":"11720891611027180364"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["\n","\n","# Compute average normalized MSE for the first 100 batches\n","def compute_average_mse(dataloader):\n","    mse_values = []\n","    count = 0\n","\n","    for data in dataloader:\n","        if count >= 10000:\n","            break\n","\n","        # Generate random \"predictions\" just for the sake of the example\n","        # In a real scenario, these would be the model's output\n","        reconstructed_batch, _, _ = vae(data.to(device))\n","\n","        mse,mse2 = normalized_mse(data.cpu(), reconstructed_batch.cpu())\n","        mse_values.append(mse.item())\n","\n","        count += 1\n","\n","    average_mse = sum(mse_values) / len(mse_values)\n","    print(\"Average Normalized MSE:\", average_mse)\n","\n"],"metadata":{"id":"6--M7y5CEymh","executionInfo":{"status":"ok","timestamp":1729166727197,"user_tz":-120,"elapsed":5,"user":{"displayName":"Valerian Fourel","userId":"11720891611027180364"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["# Train the VAE\n","train_vae(vae, dataLoaderElevation, num_epochs, learning_rateElevation )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vp7H1p9Liuv7","outputId":"19ccb6e7-593c-452e-d7e8-db6acffd2e4a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-6-59023ef0d4a7>:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  fullArray = torch.load(self.file_path+ID+self.file_extension)\n","<ipython-input-6-59023ef0d4a7>:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  fullArray = torch.load(self.file_path+ID+self.file_extension)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/3] - Progress: 3066/30667 - Total Loss: 1825419919.6060014, 0.0060603234477083585\n","Epoch [1/3] - Progress: 6132/30667 - Total Loss: 277564664.4174821, 0.0017209542940740715\n","Epoch [1/3] - Progress: 9198/30667 - Total Loss: 170139612.14220482, 0.0017131514900740035\n","Epoch [1/3] - Progress: 12264/30667 - Total Loss: 136505340.50815395, 0.0020688852296494534\n","Epoch [1/3] - Progress: 15330/30667 - Total Loss: 116598778.03587736, 0.00213207688428029\n","Epoch [1/3] - Progress: 18396/30667 - Total Loss: 104738170.07436399, 0.0020545667398232537\n","Epoch [1/3] - Progress: 21462/30667 - Total Loss: 101816615.62622309, 0.001944859978115333\n","Epoch [1/3] - Progress: 24528/30667 - Total Loss: 112163035.96151337, 0.0019479292479513209\n","Epoch [1/3] - Progress: 27594/30667 - Total Loss: 86793130.74624918, 0.0017371546265663698\n"]}]},{"cell_type":"code","source":["torch.save(vae, 'vaeElevation3Epoch.pt')"],"metadata":{"id":"zKsjRqU6j15L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!mv vaeElevation3Epoch.pt \"/content/drive/MyDrive/Colab Notebooks/\""],"metadata":{"id":"gCwGS0ZCj6Qy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Example usage\n","# Replace `your_dataloader` with the DataLoader you have\n","compute_average_mse(dataLoaderElevation)"],"metadata":{"id":"Abx-oJQ4E3_d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_vae(vae, dataLoaderElevation, 2, learning_rateElevation )"],"metadata":{"id":"mKOCBoRjEM6f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.save(vae, 'vaeElevation5Epoch.pt')"],"metadata":{"id":"OmIo6NXPEXQX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!mv vaeElevation5Epoch.pt \"/content/drive/MyDrive/Colab Notebooks/\""],"metadata":{"id":"3wGsZNwIEZZa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Example usage\n","# Replace `your_dataloader` with the DataLoader you have\n","compute_average_mse(dataLoaderElevation)"],"metadata":{"id":"XKvHmIVLE5oo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#train_vae(vae, dataLoaderElevation, 2, learning_rateElevation )"],"metadata":{"id":"RiqWzreSvFNw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["count_parameters(vae)"],"metadata":{"id":"VK1wMRk2we8P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Part 5. Plotting the results ofthe Elevation VAE"],"metadata":{"id":"YJNZFRwjMO0x"}},{"cell_type":"code","source":["def get_random_batch(dataloader):\n","    # Get the total number of batches in the DataLoader\n","    num_batches = len(dataloader)\n","\n","    # Generate a random index to select a batch\n","    random_batch_index = torch.randint(0, 100, (1,))\n","\n","    # Iterate through the DataLoader to find the batch at the random index\n","    for i, batch in enumerate(dataloader):\n","        if i == random_batch_index:\n","            return batch\n"],"metadata":{"id":"SXwUOTKYMSCx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["randomBatch = get_random_batch(dataLoaderElevation)"],"metadata":{"id":"1MRFYys1vyDg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import matplotlib.pyplot as plt\n","\n","def compare_vae_reconstruction(vae, data_batch, device):\n","    # Set the VAE to evaluation mode\n","    vae.eval()\n","\n","    # Get the batch size and number of channels\n","    batch_size, num_channels, height, width = data_batch.size()\n","\n","    # Get the reconstructed images from the VAE\n","    with torch.no_grad():\n","        dimensions = data_batch.shape\n","        data_batch = data_batch\n","        print(dimensions)\n","\n","        reconstructed_batch, _, _ = vae(data_batch.to(device))\n","    # Convert the tensors to numpy arrays and transpose the dimensions\n","    original_images = data_batch.cpu().numpy().transpose(0,2,3,1)\n","    print(reconstructed_batch.shape)\n","    reconstructed_images = reconstructed_batch.cpu().numpy().transpose(0,2,3,1)\n","    print(reconstructed_images.shape)\n","\n","\n","    # Plot the original and reconstructed images side by side\n","    plt.figure(figsize=(100, 100))\n","    for i in range(batch_size):\n","        plt.subplot(batch_size, 16, i*16 + 1)\n","        plt.imshow(original_images[i])\n","        plt.axis('off')\n","        plt.title('Original')\n","\n","        plt.subplot(batch_size, 16, i*16 + 2)\n","        plt.imshow(reconstructed_images[i])\n","        plt.axis('off')\n","        plt.title('Reconstructed')\n","\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","# Usage\n","# Assuming 'vae' is your trained Variational Autoencoder model\n","# and 'data_batch' is your batch of input images\n","# 'device' should be the device on which your model is (e.g., 'cuda' or 'cpu')\n","compare_vae_reconstruction(vae, randomBatch, device)\n"],"metadata":{"id":"esrxPcRpv07V"},"execution_count":null,"outputs":[]}]}